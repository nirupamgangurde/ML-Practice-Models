{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a07cfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nirup\\Desktop\\Neural_Network\\env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4e5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic multi-label data\n",
    "X, y = make_multilabel_classification(n_samples=1000, n_features=20, n_classes=5, n_labels=2, random_state=42) #This will generate a dataset with 1000 samples, 20 features, and 5 classes, where each sample can belong to multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18cb743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522a8a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nirup\\Desktop\\Neural_Network\\env\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nirup\\Desktop\\Neural_Network\\env\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26b22597910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # Split the data into training and validation sets\n",
    "\n",
    "# Define a simple DNN model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=20, activation='relu'),\n",
    "    Dense(5, activation='sigmoid')  # 5 labels\n",
    "]) # The output layer uses sigmoid activation for multi-label classification\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d8cf54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02578093, 0.67672664, 0.9823122 , 0.24908255, 0.06403375],\n",
       "       [0.01045672, 0.73873997, 0.69372004, 0.36369133, 0.53608286],\n",
       "       [0.41757178, 0.57487947, 0.7272643 , 0.44635072, 0.1062218 ],\n",
       "       [0.06357594, 0.3663895 , 0.5534002 , 0.5492387 , 0.02769893],\n",
       "       [0.28916037, 0.855873  , 0.3108535 , 0.4189864 , 0.01361427],\n",
       "       [0.9034678 , 0.66960853, 0.31976542, 0.20428815, 0.04613649],\n",
       "       [0.36716416, 0.06180689, 0.79312295, 0.6763623 , 0.85165864],\n",
       "       [0.2651025 , 0.48760405, 0.9933653 , 0.21358591, 0.18682338],\n",
       "       [0.31379995, 0.65185076, 0.22457738, 0.64474684, 0.03173888],\n",
       "       [0.13847218, 0.23831162, 0.34334904, 0.2926379 , 0.83392835],\n",
       "       [0.06403661, 0.5269497 , 0.04978414, 0.03504786, 0.12480785],\n",
       "       [0.63865334, 0.36747596, 0.7717239 , 0.03766093, 0.04781377],\n",
       "       [0.8628452 , 0.35699904, 0.24490432, 0.8402396 , 0.19876693],\n",
       "       [0.60192764, 0.17455739, 0.82435226, 0.26866016, 0.02616302],\n",
       "       [0.11801277, 0.98574054, 0.4321379 , 0.01718911, 0.00588441],\n",
       "       [0.22954395, 0.61167866, 0.79413986, 0.2115823 , 0.19104812],\n",
       "       [0.25982177, 0.92832613, 0.38196447, 0.52301204, 0.08594781],\n",
       "       [0.40637517, 0.37633538, 0.88855207, 0.07041139, 0.19359368],\n",
       "       [0.2592058 , 0.23556219, 0.9877364 , 0.27867836, 0.03493359],\n",
       "       [0.53759694, 0.19878344, 0.96039975, 0.5348283 , 0.12993026],\n",
       "       [0.5025198 , 0.315446  , 0.8122819 , 0.34826314, 0.10663648],\n",
       "       [0.02033469, 0.05791023, 0.9816072 , 0.08154071, 0.5002727 ],\n",
       "       [0.20404169, 0.14036562, 0.53991634, 0.453185  , 0.9458874 ],\n",
       "       [0.42920566, 0.973453  , 0.40188998, 0.06390822, 0.0468451 ],\n",
       "       [0.01821837, 0.4161818 , 0.05036425, 0.9627508 , 0.38982955],\n",
       "       [0.09587164, 0.6878737 , 0.8949149 , 0.6046487 , 0.0202923 ],\n",
       "       [0.08800963, 0.9354034 , 0.24411061, 0.39251816, 0.09421393],\n",
       "       [0.75761247, 0.5769167 , 0.47407785, 0.24378917, 0.2686711 ],\n",
       "       [0.01934917, 0.66081226, 0.48457903, 0.6511234 , 0.02578675],\n",
       "       [0.6824473 , 0.6917349 , 0.04188077, 0.86208636, 0.344358  ],\n",
       "       [0.10210428, 0.09704234, 0.98639554, 0.05377245, 0.07114128],\n",
       "       [0.9877395 , 0.16635375, 0.14770621, 0.07901184, 0.03620119],\n",
       "       [0.31638965, 0.2506797 , 0.4266437 , 0.85958606, 0.5177672 ],\n",
       "       [0.05590905, 0.9977402 , 0.03164648, 0.14620963, 0.08310395],\n",
       "       [0.1113743 , 0.16405939, 0.6894869 , 0.64639735, 0.097051  ],\n",
       "       [0.15433219, 0.8344198 , 0.8166892 , 0.538543  , 0.11476444],\n",
       "       [0.88881326, 0.30187327, 0.2709591 , 0.49276176, 0.1100972 ],\n",
       "       [0.06666639, 0.77767307, 0.7488983 , 0.19744737, 0.05895155],\n",
       "       [0.03809131, 0.76863855, 0.9544013 , 0.17485109, 0.04326389],\n",
       "       [0.02770774, 0.38095948, 0.6808847 , 0.90615827, 0.2520067 ],\n",
       "       [0.63423944, 0.97389454, 0.02950911, 0.21869574, 0.17009059],\n",
       "       [0.74429727, 0.5321553 , 0.8465691 , 0.53098   , 0.23169212],\n",
       "       [0.20884879, 0.92084813, 0.08554839, 0.40933645, 0.11969891],\n",
       "       [0.05621365, 0.7955723 , 0.01217989, 0.908211  , 0.07803588],\n",
       "       [0.4157361 , 0.40768358, 0.3867472 , 0.5799472 , 0.49658495],\n",
       "       [0.5711693 , 0.6330644 , 0.39942956, 0.09196945, 0.05009945],\n",
       "       [0.94512457, 0.93647635, 0.3001681 , 0.00755111, 0.02246399],\n",
       "       [0.5998462 , 0.28201783, 0.869503  , 0.840484  , 0.07028272],\n",
       "       [0.03794907, 0.9893836 , 0.10901376, 0.0896293 , 0.02722097],\n",
       "       [0.9560503 , 0.36638853, 0.8295402 , 0.2290803 , 0.0193533 ],\n",
       "       [0.10383981, 0.05207435, 0.03845854, 0.03869759, 0.94889605],\n",
       "       [0.09079505, 0.1106241 , 0.5704581 , 0.89273095, 0.5243279 ],\n",
       "       [0.60494435, 0.10855448, 0.73093474, 0.9470809 , 0.01034511],\n",
       "       [0.43693352, 0.6307174 , 0.97397304, 0.06641022, 0.02015509],\n",
       "       [0.4635689 , 0.99798137, 0.00819814, 0.0183851 , 0.02325666],\n",
       "       [0.03078826, 0.9637812 , 0.28952962, 0.05057431, 0.35792714],\n",
       "       [0.6140493 , 0.60543245, 0.8654087 , 0.26090512, 0.02663365],\n",
       "       [0.07300798, 0.14775082, 0.2695486 , 0.86486894, 0.02640952],\n",
       "       [0.84525126, 0.14819568, 0.10807059, 0.17763934, 0.11051341],\n",
       "       [0.11453006, 0.3373017 , 0.8624926 , 0.1540228 , 0.25536728],\n",
       "       [0.01494338, 0.22296238, 0.00256161, 0.97657347, 0.3866004 ],\n",
       "       [0.07352892, 0.5302087 , 0.7123509 , 0.89906776, 0.06708405],\n",
       "       [0.9524835 , 0.52488285, 0.2576776 , 0.02017876, 0.03437391],\n",
       "       [0.03807715, 0.53486824, 0.0562669 , 0.95854497, 0.1771593 ],\n",
       "       [0.07642173, 0.15525192, 0.9182733 , 0.24777919, 0.4019908 ],\n",
       "       [0.12686348, 0.28805164, 0.9633765 , 0.46200395, 0.12270554],\n",
       "       [0.00806725, 0.1699838 , 0.2480347 , 0.9512124 , 0.15014134],\n",
       "       [0.09654488, 0.99736935, 0.02254955, 0.03103431, 0.01581707],\n",
       "       [0.78997386, 0.88991517, 0.06034116, 0.10317483, 0.4783224 ],\n",
       "       [0.07474638, 0.3755013 , 0.6795479 , 0.06582554, 0.42594978],\n",
       "       [0.20698616, 0.427331  , 0.9876816 , 0.21988645, 0.10239436],\n",
       "       [0.04588712, 0.29453057, 0.9933029 , 0.13760766, 0.07662189],\n",
       "       [0.3072002 , 0.1229875 , 0.67611897, 0.08292364, 0.07497545],\n",
       "       [0.48643386, 0.08409213, 0.3013007 , 0.27839974, 0.18715285],\n",
       "       [0.08695941, 0.9086175 , 0.02526619, 0.75205904, 0.0835996 ],\n",
       "       [0.33056387, 0.11024478, 0.47905195, 0.31397885, 0.42424896],\n",
       "       [0.7540627 , 0.15323232, 0.17889263, 0.27451816, 0.06832025],\n",
       "       [0.23661621, 0.18052293, 0.9364833 , 0.7841408 , 0.2962968 ],\n",
       "       [0.17411155, 0.13687126, 0.9854122 , 0.13962461, 0.06042515],\n",
       "       [0.7770258 , 0.04802652, 0.42643327, 0.08170956, 0.6271227 ],\n",
       "       [0.34412864, 0.21093766, 0.15650247, 0.9797385 , 0.0708293 ],\n",
       "       [0.5164453 , 0.64288974, 0.0510202 , 0.37217122, 0.15155305],\n",
       "       [0.15428935, 0.76933086, 0.93754476, 0.38089696, 0.03311608],\n",
       "       [0.23939925, 0.9976947 , 0.05709696, 0.19722988, 0.04573205],\n",
       "       [0.5599772 , 0.1723711 , 0.5709776 , 0.08901732, 0.0525097 ],\n",
       "       [0.00631981, 0.14797893, 0.03879166, 0.96637166, 0.36311764],\n",
       "       [0.71327585, 0.5182782 , 0.14666073, 0.7264988 , 0.4947854 ],\n",
       "       [0.28865752, 0.07311361, 0.18714574, 0.04808373, 0.3477946 ],\n",
       "       [0.3084456 , 0.9715654 , 0.13767096, 0.40071854, 0.28084332],\n",
       "       [0.1684221 , 0.8464822 , 0.15806444, 0.39543116, 0.274899  ],\n",
       "       [0.06439933, 0.21351565, 0.26220042, 0.78499943, 0.27361894],\n",
       "       [0.05543777, 0.15894063, 0.06543662, 0.97511613, 0.44593593],\n",
       "       [0.7584342 , 0.20609958, 0.7027772 , 0.3947737 , 0.3974852 ],\n",
       "       [0.45152432, 0.25640678, 0.15019813, 0.20785071, 0.0259775 ],\n",
       "       [0.01069239, 0.5795637 , 0.04287625, 0.53016627, 0.13064599],\n",
       "       [0.05499203, 0.96798044, 0.28263706, 0.19633344, 0.12491857],\n",
       "       [0.01239811, 0.35197118, 0.9489287 , 0.7938024 , 0.05823969],\n",
       "       [0.01020183, 0.99147004, 0.650113  , 0.20624717, 0.00754829],\n",
       "       [0.9617163 , 0.70369023, 0.35550493, 0.03862422, 0.02785305],\n",
       "       [0.7329653 , 0.96406394, 0.18403861, 0.2925219 , 0.18900472],\n",
       "       [0.66023713, 0.78191906, 0.13808799, 0.09545222, 0.06123224],\n",
       "       [0.0038325 , 0.9936528 , 0.13136175, 0.06996055, 0.11165873],\n",
       "       [0.4237996 , 0.9405364 , 0.01752509, 0.35265228, 0.10781799],\n",
       "       [0.02953864, 0.12152065, 0.06845184, 0.12572962, 0.97704387],\n",
       "       [0.940954  , 0.14318493, 0.08969636, 0.6813384 , 0.12029047],\n",
       "       [0.06504259, 0.97244346, 0.12228072, 0.15534516, 0.08097509],\n",
       "       [0.69246554, 0.9507674 , 0.4397632 , 0.02871073, 0.03202803],\n",
       "       [0.38019702, 0.48634136, 0.72981894, 0.45799807, 0.38026622],\n",
       "       [0.39624435, 0.4445961 , 0.91036934, 0.6982628 , 0.36564502],\n",
       "       [0.14740504, 0.94458777, 0.5801798 , 0.36931947, 0.05861607],\n",
       "       [0.08505379, 0.0017336 , 0.9153181 , 0.8115972 , 0.5922269 ],\n",
       "       [0.5594309 , 0.84154695, 0.89164627, 0.01673801, 0.07545628],\n",
       "       [0.7763348 , 0.5754391 , 0.41291276, 0.6023319 , 0.47095725],\n",
       "       [0.02974712, 0.9654704 , 0.67590606, 0.2011553 , 0.1764589 ],\n",
       "       [0.07746001, 0.31107673, 0.15739872, 0.2898273 , 0.24280082],\n",
       "       [0.99676037, 0.52214825, 0.04602831, 0.01331418, 0.00739642],\n",
       "       [0.61479867, 0.7272679 , 0.4981649 , 0.23683017, 0.7362288 ],\n",
       "       [0.5221533 , 0.6224588 , 0.5085574 , 0.35551897, 0.63389313],\n",
       "       [0.18046854, 0.69370365, 0.14616123, 0.33742967, 0.01837491],\n",
       "       [0.02510834, 0.08924624, 0.9571474 , 0.05938198, 0.09868728],\n",
       "       [0.22366899, 0.21342205, 0.4125779 , 0.6181851 , 0.97450155],\n",
       "       [0.4902367 , 0.5877171 , 0.24640964, 0.6503948 , 0.04576125],\n",
       "       [0.02368034, 0.3098953 , 0.59432775, 0.6948969 , 0.3726324 ],\n",
       "       [0.1157909 , 0.86045426, 0.8640461 , 0.07814109, 0.12751858],\n",
       "       [0.43599737, 0.90748286, 0.03733119, 0.15529668, 0.03343724],\n",
       "       [0.09637403, 0.07246694, 0.036947  , 0.99011564, 0.21337211],\n",
       "       [0.08752386, 0.47887936, 0.3818025 , 0.4050205 , 0.09183776],\n",
       "       [0.31012073, 0.94555205, 0.34608468, 0.18655522, 0.01409359],\n",
       "       [0.508468  , 0.5251052 , 0.94753283, 0.01433852, 0.1227155 ],\n",
       "       [0.79101443, 0.5863667 , 0.04023432, 0.5376196 , 0.01538027],\n",
       "       [0.9818692 , 0.3504385 , 0.34165564, 0.09775736, 0.01570366],\n",
       "       [0.6518231 , 0.2950465 , 0.21094583, 0.35632527, 0.06583975],\n",
       "       [0.01891334, 0.994544  , 0.3772624 , 0.19578752, 0.02931461],\n",
       "       [0.37826797, 0.21248746, 0.4107017 , 0.14661016, 0.17906757],\n",
       "       [0.2268759 , 0.06849994, 0.9955861 , 0.24896888, 0.08048735],\n",
       "       [0.21411961, 0.9279158 , 0.5154284 , 0.35662082, 0.07593691],\n",
       "       [0.75236297, 0.76807815, 0.09313945, 0.5204909 , 0.12273011],\n",
       "       [0.28760606, 0.1682628 , 0.8939739 , 0.7361765 , 0.23012286],\n",
       "       [0.03520307, 0.76586735, 0.9109389 , 0.05941056, 0.07554489],\n",
       "       [0.58957034, 0.062418  , 0.84917057, 0.06559677, 0.1216373 ],\n",
       "       [0.01432227, 0.99767256, 0.03003198, 0.14718647, 0.00829964],\n",
       "       [0.18120535, 0.2156692 , 0.4617698 , 0.26948217, 0.8777566 ],\n",
       "       [0.5083549 , 0.6463308 , 0.8914319 , 0.22562408, 0.3198787 ],\n",
       "       [0.22262049, 0.13020866, 0.38936895, 0.47070605, 0.054248  ],\n",
       "       [0.09937064, 0.5799212 , 0.02741091, 0.8060389 , 0.4665444 ],\n",
       "       [0.08817327, 0.95725083, 0.29109347, 0.6925672 , 0.0157607 ],\n",
       "       [0.20187679, 0.04377566, 0.16944365, 0.3754616 , 0.235548  ],\n",
       "       [0.7694245 , 0.07461872, 0.80707943, 0.25804555, 0.18084624],\n",
       "       [0.9800443 , 0.6805242 , 0.14040424, 0.07102375, 0.08787157],\n",
       "       [0.05851915, 0.43417543, 0.951327  , 0.8059283 , 0.18586847],\n",
       "       [0.10792468, 0.13731249, 0.98250496, 0.05263644, 0.31169322],\n",
       "       [0.03331172, 0.98647004, 0.04264669, 0.28541088, 0.13825172],\n",
       "       [0.11215362, 0.04415813, 0.7064714 , 0.6658964 , 0.38434264],\n",
       "       [0.0502926 , 0.67637235, 0.79504216, 0.22263624, 0.26194492],\n",
       "       [0.48136106, 0.61159056, 0.54848707, 0.3784225 , 0.06164921],\n",
       "       [0.33018738, 0.11225832, 0.7195205 , 0.72793996, 0.19598182],\n",
       "       [0.16599771, 0.1873587 , 0.98443586, 0.32220703, 0.08996154],\n",
       "       [0.03772574, 0.90907073, 0.8490094 , 0.1858897 , 0.06130508],\n",
       "       [0.01703952, 0.21301812, 0.27301162, 0.98650056, 0.05718423],\n",
       "       [0.6602025 , 0.24883048, 0.5133188 , 0.08117891, 0.17640632],\n",
       "       [0.02316759, 0.9905971 , 0.09607875, 0.19981812, 0.12395602],\n",
       "       [0.04105107, 0.11511412, 0.03667166, 0.57788575, 0.91469675],\n",
       "       [0.24568646, 0.710864  , 0.18893312, 0.43479493, 0.23550966],\n",
       "       [0.03112581, 0.9979464 , 0.08645137, 0.18726169, 0.04137814],\n",
       "       [0.5577472 , 0.7262855 , 0.6664021 , 0.03678077, 0.1308066 ],\n",
       "       [0.06386925, 0.2788671 , 0.12068718, 0.36329588, 0.790866  ],\n",
       "       [0.00519827, 0.97024363, 0.00786606, 0.54615325, 0.06842216],\n",
       "       [0.06120951, 0.30113763, 0.3677904 , 0.78875405, 0.36285985],\n",
       "       [0.041108  , 0.05467245, 0.9646394 , 0.03661017, 0.17871651],\n",
       "       [0.70683646, 0.7025735 , 0.24674559, 0.37060854, 0.24445365],\n",
       "       [0.01598754, 0.9904022 , 0.05804162, 0.15861987, 0.04379936],\n",
       "       [0.05210551, 0.40123332, 0.13045873, 0.92474025, 0.4772439 ],\n",
       "       [0.40402877, 0.568407  , 0.717389  , 0.14176087, 0.47839284],\n",
       "       [0.8876217 , 0.92737705, 0.0291176 , 0.04248544, 0.05786458],\n",
       "       [0.01896904, 0.99409926, 0.08556019, 0.15925583, 0.04061445],\n",
       "       [0.7192858 , 0.5720426 , 0.18296148, 0.20503604, 0.05626426],\n",
       "       [0.13030258, 0.53949946, 0.10812023, 0.90227437, 0.35145617],\n",
       "       [0.16257995, 0.95318437, 0.00912713, 0.41474426, 0.14828153],\n",
       "       [0.6284693 , 0.38678744, 0.9256829 , 0.24833524, 0.05423586],\n",
       "       [0.6156952 , 0.81173635, 0.07595553, 0.7068091 , 0.5300448 ],\n",
       "       [0.16810928, 0.2767519 , 0.9035962 , 0.1118411 , 0.3884652 ],\n",
       "       [0.00363238, 0.9640566 , 0.17814484, 0.558859  , 0.04880107],\n",
       "       [0.28037152, 0.8645656 , 0.21497104, 0.47256178, 0.11901271],\n",
       "       [0.04506556, 0.96814126, 0.41994056, 0.06603833, 0.02843303],\n",
       "       [0.10532494, 0.379097  , 0.40433812, 0.6585688 , 0.02602631],\n",
       "       [0.6883188 , 0.19866931, 0.7981759 , 0.26424533, 0.4085213 ],\n",
       "       [0.07465269, 0.6802228 , 0.9877185 , 0.2064315 , 0.00604839],\n",
       "       [0.8065605 , 0.57044363, 0.3351059 , 0.728182  , 0.3846845 ],\n",
       "       [0.04243394, 0.8599966 , 0.6937547 , 0.11429333, 0.17305385],\n",
       "       [0.5735372 , 0.8958482 , 0.40947542, 0.6716509 , 0.02934239],\n",
       "       [0.61218023, 0.19634113, 0.89079666, 0.16728185, 0.10570671],\n",
       "       [0.04552903, 0.12265642, 0.6270554 , 0.12969136, 0.9767669 ],\n",
       "       [0.3415482 , 0.08569555, 0.92787397, 0.85736   , 0.20085053],\n",
       "       [0.16293089, 0.8839427 , 0.91156113, 0.16408351, 0.21298364],\n",
       "       [0.57530284, 0.98830324, 0.02123455, 0.26007703, 0.21600553],\n",
       "       [0.17548162, 0.42638886, 0.8342719 , 0.75454986, 0.09752163],\n",
       "       [0.03758835, 0.31934556, 0.9896543 , 0.12735835, 0.05657071],\n",
       "       [0.985225  , 0.8508165 , 0.1293489 , 0.01826999, 0.04201008],\n",
       "       [0.25078332, 0.9195536 , 0.53629243, 0.25597835, 0.12595488],\n",
       "       [0.1783729 , 0.21121441, 0.97156954, 0.22372976, 0.01757768]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict probabilities\n",
    "y_pred_prob = model.predict(X_val)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d4dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Best Threshold = 0.54, F1-score = 0.7563\n",
      "Label 1: Best Threshold = 0.37, F1-score = 0.8108\n",
      "Label 2: Best Threshold = 0.35, F1-score = 0.8113\n",
      "Label 3: Best Threshold = 0.39, F1-score = 0.7467\n",
      "Label 4: Best Threshold = 0.41, F1-score = 0.6970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5399999999999998,\n",
       " 0.3699999999999999,\n",
       " 0.34999999999999987,\n",
       " 0.3899999999999999,\n",
       " 0.4099999999999998]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds = []\n",
    "for i in range(y.shape[1]):  # for each label\n",
    "    best_thresh = 0.5 # this is the default threshold\n",
    "    best_f1 = 0.0 # initialize best F1 score\n",
    "    for t in np.arange(0.1, 0.9, 0.01): #this is the range of thresholds to test\n",
    "        y_pred_bin = (y_pred_prob[:, i] > t).astype(int) # this will convert the probabilities to binary predictions based on the threshold\n",
    "        f1 = f1_score(y_val[:, i], y_pred_bin) #this will calculate the F1 score for the current threshold\n",
    "        if f1 > best_f1: # this will check if the current F1 score is better than the best one\n",
    "            best_f1 = f1 # this will update the best F1 score\n",
    "            best_thresh = t # this will update the best threshold\n",
    "    thresholds.append(best_thresh) # this will store the best threshold for the current label\n",
    "    print(f\"Label {i}: Best Threshold = {best_thresh:.2f}, F1-score = {best_f1:.4f}\") # this will print the best threshold and F1 score for each label\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5cfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1-score: 0.7802340702210663\n",
      "Macro F1-score: 0.7644140900345118\n"
     ]
    }
   ],
   "source": [
    "y_pred_final = np.zeros_like(y_pred_prob) # this will create an array of zeros with the same shape as y_pred_prob\n",
    "for i, t in enumerate(thresholds): # this will loop through each label and its corresponding threshold\n",
    "    y_pred_final[:, i] = (y_pred_prob[:, i] > t).astype(int) # this will convert the probabilities to binary predictions based on the threshold\n",
    "\n",
    "# Overall micro and macro F1\n",
    "print(\"Micro F1-score:\", f1_score(y_val, y_pred_final, average='micro')) # this will calculate the micro F1 score\n",
    "print(\"Macro F1-score:\", f1_score(y_val, y_pred_final, average='macro')) # this will calculate the macro F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7bd0277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 1.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db814dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 1.],\n",
       "       [0., 1., 1., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##thresholds = [0.27, 0.49, 0.41, 0.28, 0.31]\n",
    "def multilabel_predict(X, model, thresholds):\n",
    "    prob = model.predict(X)\n",
    "    pred = np.zeros_like(prob)\n",
    "    for i, t in enumerate(thresholds):\n",
    "        pred[:, i] = (prob[:, i] > t).astype(int)\n",
    "    return pred\n",
    "y_pred_final = multilabel_predict(X_val, model, thresholds)\n",
    "y_pred_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15352d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: ['Label_1', 'Label_2']\n",
      "Sample 1: ['Label_1', 'Label_2', 'Label_4']\n",
      "Sample 2: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 3: ['Label_2', 'Label_3']\n",
      "Sample 4: ['Label_1', 'Label_3']\n",
      "Sample 5: ['Label_0', 'Label_1']\n",
      "Sample 6: ['Label_2', 'Label_3', 'Label_4']\n",
      "Sample 7: ['Label_1', 'Label_2']\n",
      "Sample 8: ['Label_1', 'Label_3']\n",
      "Sample 9: ['Label_4']\n",
      "Sample 10: ['Label_1']\n",
      "Sample 11: ['Label_0', 'Label_2']\n",
      "Sample 12: ['Label_0', 'Label_3']\n",
      "Sample 13: ['Label_0', 'Label_2']\n",
      "Sample 14: ['Label_1', 'Label_2']\n",
      "Sample 15: ['Label_1', 'Label_2']\n",
      "Sample 16: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 17: ['Label_1', 'Label_2']\n",
      "Sample 18: ['Label_2']\n",
      "Sample 19: ['Label_2', 'Label_3']\n",
      "Sample 20: ['Label_2']\n",
      "Sample 21: ['Label_2', 'Label_4']\n",
      "Sample 22: ['Label_2', 'Label_3', 'Label_4']\n",
      "Sample 23: ['Label_1', 'Label_2']\n",
      "Sample 24: ['Label_1', 'Label_3']\n",
      "Sample 25: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 26: ['Label_1', 'Label_3']\n",
      "Sample 27: ['Label_0', 'Label_1', 'Label_2']\n",
      "Sample 28: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 29: ['Label_0', 'Label_1', 'Label_3']\n",
      "Sample 30: ['Label_2']\n",
      "Sample 31: ['Label_0']\n",
      "Sample 32: ['Label_2', 'Label_3', 'Label_4']\n",
      "Sample 33: ['Label_1']\n",
      "Sample 34: ['Label_2', 'Label_3']\n",
      "Sample 35: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 36: ['Label_0', 'Label_3']\n",
      "Sample 37: ['Label_1', 'Label_2']\n",
      "Sample 38: ['Label_1', 'Label_2']\n",
      "Sample 39: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 40: ['Label_0', 'Label_1']\n",
      "Sample 41: ['Label_0', 'Label_1', 'Label_2', 'Label_3']\n",
      "Sample 42: ['Label_1', 'Label_3']\n",
      "Sample 43: ['Label_1', 'Label_3']\n",
      "Sample 44: ['Label_1', 'Label_2', 'Label_3', 'Label_4']\n",
      "Sample 45: ['Label_0', 'Label_1', 'Label_2']\n",
      "Sample 46: ['Label_0', 'Label_1']\n",
      "Sample 47: ['Label_0', 'Label_2', 'Label_3']\n",
      "Sample 48: ['Label_1']\n",
      "Sample 49: ['Label_0', 'Label_2']\n",
      "Sample 50: ['Label_4']\n",
      "Sample 51: ['Label_2', 'Label_3', 'Label_4']\n",
      "Sample 52: ['Label_0', 'Label_2', 'Label_3']\n",
      "Sample 53: ['Label_1', 'Label_2']\n",
      "Sample 54: ['Label_1']\n",
      "Sample 55: ['Label_1']\n",
      "Sample 56: ['Label_0', 'Label_1', 'Label_2']\n",
      "Sample 57: ['Label_3']\n",
      "Sample 58: ['Label_0']\n",
      "Sample 59: ['Label_2']\n",
      "Sample 60: ['Label_3']\n",
      "Sample 61: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 62: ['Label_0', 'Label_1']\n",
      "Sample 63: ['Label_1', 'Label_3']\n",
      "Sample 64: ['Label_2']\n",
      "Sample 65: ['Label_2', 'Label_3']\n",
      "Sample 66: ['Label_3']\n",
      "Sample 67: ['Label_1']\n",
      "Sample 68: ['Label_0', 'Label_1', 'Label_4']\n",
      "Sample 69: ['Label_1', 'Label_2', 'Label_4']\n",
      "Sample 70: ['Label_1', 'Label_2']\n",
      "Sample 71: ['Label_2']\n",
      "Sample 72: ['Label_2']\n",
      "Sample 73: []\n",
      "Sample 74: ['Label_1', 'Label_3']\n",
      "Sample 75: ['Label_2', 'Label_4']\n",
      "Sample 76: ['Label_0']\n",
      "Sample 77: ['Label_2', 'Label_3']\n",
      "Sample 78: ['Label_2']\n",
      "Sample 79: ['Label_0', 'Label_2', 'Label_4']\n",
      "Sample 80: ['Label_3']\n",
      "Sample 81: ['Label_1']\n",
      "Sample 82: ['Label_1', 'Label_2']\n",
      "Sample 83: ['Label_1']\n",
      "Sample 84: ['Label_0', 'Label_2']\n",
      "Sample 85: ['Label_3']\n",
      "Sample 86: ['Label_0', 'Label_1', 'Label_3', 'Label_4']\n",
      "Sample 87: []\n",
      "Sample 88: ['Label_1', 'Label_3']\n",
      "Sample 89: ['Label_1', 'Label_3']\n",
      "Sample 90: ['Label_3']\n",
      "Sample 91: ['Label_3', 'Label_4']\n",
      "Sample 92: ['Label_0', 'Label_2', 'Label_3']\n",
      "Sample 93: []\n",
      "Sample 94: ['Label_1', 'Label_3']\n",
      "Sample 95: ['Label_1']\n",
      "Sample 96: ['Label_2', 'Label_3']\n",
      "Sample 97: ['Label_1', 'Label_2']\n",
      "Sample 98: ['Label_0', 'Label_1', 'Label_2']\n",
      "Sample 99: ['Label_0', 'Label_1']\n",
      "Sample 100: ['Label_0', 'Label_1']\n",
      "Sample 101: ['Label_1']\n",
      "Sample 102: ['Label_1']\n",
      "Sample 103: ['Label_4']\n",
      "Sample 104: ['Label_0', 'Label_3']\n",
      "Sample 105: ['Label_1']\n",
      "Sample 106: ['Label_0', 'Label_1', 'Label_2']\n",
      "Sample 107: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 108: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 109: ['Label_1', 'Label_2']\n",
      "Sample 110: ['Label_2', 'Label_3', 'Label_4']\n",
      "Sample 111: ['Label_0', 'Label_1', 'Label_2']\n",
      "Sample 112: ['Label_0', 'Label_1', 'Label_2', 'Label_3', 'Label_4']\n",
      "Sample 113: ['Label_1', 'Label_2']\n",
      "Sample 114: []\n",
      "Sample 115: ['Label_0', 'Label_1']\n",
      "Sample 116: ['Label_0', 'Label_1', 'Label_2', 'Label_4']\n",
      "Sample 117: ['Label_1', 'Label_2', 'Label_4']\n",
      "Sample 118: ['Label_1']\n",
      "Sample 119: ['Label_2']\n",
      "Sample 120: ['Label_2', 'Label_3', 'Label_4']\n",
      "Sample 121: ['Label_1', 'Label_3']\n",
      "Sample 122: ['Label_2', 'Label_3']\n",
      "Sample 123: ['Label_1', 'Label_2']\n",
      "Sample 124: ['Label_1']\n",
      "Sample 125: ['Label_3']\n",
      "Sample 126: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 127: ['Label_1']\n",
      "Sample 128: ['Label_1', 'Label_2']\n",
      "Sample 129: ['Label_0', 'Label_1', 'Label_3']\n",
      "Sample 130: ['Label_0']\n",
      "Sample 131: ['Label_0']\n",
      "Sample 132: ['Label_1', 'Label_2']\n",
      "Sample 133: ['Label_2']\n",
      "Sample 134: ['Label_2']\n",
      "Sample 135: ['Label_1', 'Label_2']\n",
      "Sample 136: ['Label_0', 'Label_1', 'Label_3']\n",
      "Sample 137: ['Label_2', 'Label_3']\n",
      "Sample 138: ['Label_1', 'Label_2']\n",
      "Sample 139: ['Label_0', 'Label_2']\n",
      "Sample 140: ['Label_1']\n",
      "Sample 141: ['Label_2', 'Label_4']\n",
      "Sample 142: ['Label_1', 'Label_2']\n",
      "Sample 143: ['Label_2', 'Label_3']\n",
      "Sample 144: ['Label_1', 'Label_3', 'Label_4']\n",
      "Sample 145: ['Label_1', 'Label_3']\n",
      "Sample 146: []\n",
      "Sample 147: ['Label_0', 'Label_2']\n",
      "Sample 148: ['Label_0', 'Label_1']\n",
      "Sample 149: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 150: ['Label_2']\n",
      "Sample 151: ['Label_1']\n",
      "Sample 152: ['Label_2', 'Label_3']\n",
      "Sample 153: ['Label_1', 'Label_2']\n",
      "Sample 154: ['Label_1', 'Label_2']\n",
      "Sample 155: ['Label_2', 'Label_3']\n",
      "Sample 156: ['Label_2']\n",
      "Sample 157: ['Label_1', 'Label_2']\n",
      "Sample 158: ['Label_3']\n",
      "Sample 159: ['Label_0', 'Label_2']\n",
      "Sample 160: ['Label_1']\n",
      "Sample 161: ['Label_3', 'Label_4']\n",
      "Sample 162: ['Label_1', 'Label_3']\n",
      "Sample 163: ['Label_1']\n",
      "Sample 164: ['Label_0', 'Label_1', 'Label_2']\n",
      "Sample 165: ['Label_4']\n",
      "Sample 166: ['Label_1', 'Label_3']\n",
      "Sample 167: ['Label_2', 'Label_3']\n",
      "Sample 168: ['Label_2']\n",
      "Sample 169: ['Label_0', 'Label_1']\n",
      "Sample 170: ['Label_1']\n",
      "Sample 171: ['Label_1', 'Label_3', 'Label_4']\n",
      "Sample 172: ['Label_1', 'Label_2', 'Label_4']\n",
      "Sample 173: ['Label_0', 'Label_1']\n",
      "Sample 174: ['Label_1']\n",
      "Sample 175: ['Label_0', 'Label_1']\n",
      "Sample 176: ['Label_1', 'Label_3']\n",
      "Sample 177: ['Label_1', 'Label_3']\n",
      "Sample 178: ['Label_0', 'Label_1', 'Label_2']\n",
      "Sample 179: ['Label_0', 'Label_1', 'Label_3', 'Label_4']\n",
      "Sample 180: ['Label_2']\n",
      "Sample 181: ['Label_1', 'Label_3']\n",
      "Sample 182: ['Label_1', 'Label_3']\n",
      "Sample 183: ['Label_1', 'Label_2']\n",
      "Sample 184: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 185: ['Label_0', 'Label_2']\n",
      "Sample 186: ['Label_1', 'Label_2']\n",
      "Sample 187: ['Label_0', 'Label_1', 'Label_3']\n",
      "Sample 188: ['Label_1', 'Label_2']\n",
      "Sample 189: ['Label_0', 'Label_1', 'Label_2', 'Label_3']\n",
      "Sample 190: ['Label_0', 'Label_2']\n",
      "Sample 191: ['Label_2', 'Label_4']\n",
      "Sample 192: ['Label_2', 'Label_3']\n",
      "Sample 193: ['Label_1', 'Label_2']\n",
      "Sample 194: ['Label_0', 'Label_1']\n",
      "Sample 195: ['Label_1', 'Label_2', 'Label_3']\n",
      "Sample 196: ['Label_2']\n",
      "Sample 197: ['Label_0', 'Label_1']\n",
      "Sample 198: ['Label_1', 'Label_2']\n",
      "Sample 199: ['Label_2']\n"
     ]
    }
   ],
   "source": [
    "label_names = [\"Label_0\", \"Label_1\", \"Label_2\", \"Label_3\", \"Label_4\"]\n",
    "readable_preds = []\n",
    "\n",
    "for row in y_pred_final:\n",
    "    labels = [label_names[i] for i, val in enumerate(row) if val == 1]\n",
    "    readable_preds.append(labels)\n",
    "\n",
    "# Display\n",
    "for i, labels in enumerate(readable_preds):\n",
    "    print(f\"Sample {i}: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39957a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
